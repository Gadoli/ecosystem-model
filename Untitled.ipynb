{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data_set():\n",
    "    data_set = LabeledSet(2)\n",
    "    sample = np.random.multivariate_normal((2, -2), 1.5 * np.identity(2), 100)\n",
    "    for i in range(sample.shape[0]):\n",
    "        data_set.add_example(sample[i], 1)\n",
    "    sample = np.random.multivariate_normal((2, 2), np.identity(2), 100)\n",
    "    for i in range(sample.shape[0]):\n",
    "        data_set.add_example(sample[i], 1)\n",
    "    sample = np.random.multivariate_normal((-2, -2), np.identity(2), 100)\n",
    "    for i in range(sample.shape[0]):\n",
    "        data_set.add_example(sample[i], 1)\n",
    "    sample = np.random.multivariate_normal((-1.5, 1.5), np.identity(2), 100)\n",
    "    for i in range(sample.shape[0]):\n",
    "        data_set.add_example(sample[i], -1)\n",
    "    return data_set\n",
    "\n",
    "def plot_frontiere(set, classifier, step=20):\n",
    "    mmax=set.x.max(0)\n",
    "    mmin=set.x.min(0)\n",
    "    x1grid,x2grid=np.meshgrid(np.linspace(mmin[0],mmax[0],step),np.linspace(mmin[1],mmax[1],step))\n",
    "    grid=np.hstack((x1grid.reshape(x1grid.size,1),x2grid.reshape(x2grid.size,1)))\n",
    "    \n",
    "    res=np.array([classifier.predict(grid[i,:]) for i in range(len(grid)) ])\n",
    "    res=res.reshape(x1grid.shape)\n",
    "    plt.contourf(x1grid, x2grid, res)#, colors=[\"red\",\"cyan\"], levels=[-1000,0,1000], linewidth=2)\n",
    "    \n",
    "def plot2DSet(data_set):\n",
    "    positive_indexes = []\n",
    "    negative_indexes = []\n",
    "    for i in range(data_set.y.shape[0]):\n",
    "        if data_set.y[i] > 0:\n",
    "            positive_indexes.append(i)\n",
    "        else:\n",
    "            negative_indexes.append(i)\n",
    "    plt.scatter(data_set.x[positive_indexes, 0], data_set.x[positive_indexes, 1], marker='x')\n",
    "    plt.scatter(data_set.x[negative_indexes, 0], data_set.x[negative_indexes, 1], marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from math import log, sqrt\n",
    "import random\n",
    "\n",
    "\n",
    "class LabeledSet:  \n",
    "    def __init__(self, input_dimension):\n",
    "        self.input_dimension=input_dimension\n",
    "        self.example_number = 0\n",
    "    \n",
    "    def add_example(self, vector, label):\n",
    "        if (self.example_number == 0):\n",
    "            self.x = np.array([vector])\n",
    "            self.y = np.array([label])\n",
    "        else:\n",
    "            self.x = np.vstack((self.x,vector))\n",
    "            self.y = np.vstack((self.y,label))\n",
    "        \n",
    "        self.example_number = self.example_number + 1\n",
    "    \n",
    "    def get_input_dimension(self):\n",
    "        return self.input_dimension\n",
    "    \n",
    "    def size(self):\n",
    "        return self.example_number\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        return self.x[i]\n",
    "    \n",
    "    def get_y(self, i):\n",
    "        return(self.y[i])\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, input_dimension):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "        \n",
    "    def predict(self, x):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def train(self, training_set):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "    \n",
    "    def TP_FP_TN_FN(self, set):\n",
    "        FN, FP, TP, TN = 0, 0, 0, 0\n",
    "        for i in range(set.size()):\n",
    "            if (self.predict(set.get_x(i)) != 1 and set.get_y(i) == 1):\n",
    "                FN += + 1\n",
    "            elif (self.predict(set.get_x(i)) == 1 and set.get_y(i) == 1):\n",
    "                TP += + 1\n",
    "            elif (self.predict(set.get_x(i)) != 1 and set.get_y(i) != 1):\n",
    "                TN += + 1\n",
    "            else:\n",
    "                FP += + 1\n",
    "        return TP, FP, TN, FN\n",
    "    \n",
    "    def precision(self, set):\n",
    "        TP, FP, TN, FN = self.TP_FP_TN_FN(set)\n",
    "        if (TP + FP != 0):\n",
    "            return TP / (TP + FP)\n",
    "        return -1\n",
    "\n",
    "    def recall(self, set, positive_recall=True):\n",
    "        TP, FP, TN, FN = self.TP_FP_TN_FN(set)\n",
    "        if (TP + FN != 0):\n",
    "            return TP / (TP + FN)\n",
    "        return -1\n",
    "    \n",
    "    def fall_out(self, set):\n",
    "        TP, FP, TN, FN = self.TP_FP_TN_FN(set)\n",
    "        return FP / (FP + TN)\n",
    "\n",
    "    def accuracy(self, set):\n",
    "        TP, FP, TN, FN = self.TP_FP_TN_FN(set)\n",
    "        return (TP + TN) / set.size()\n",
    "    \n",
    "    def f_measure(self, set, beta=1.0):\n",
    "        if beta < 0:\n",
    "            raise ValueError(\"Invalid beta: \" + str(beta))\n",
    "        precision = self.precision(set)\n",
    "        recall = self.recall(set)\n",
    "        if recall == 0 and beta * precision == 0:\n",
    "            return 0\n",
    "        return ((1 + beta ** 2) * precision * recall) / (precision * (beta ** 2) + recall)\n",
    "\n",
    "class Regression(Classifier):\n",
    "    def regression_predict(self, x):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return 1 if self.regression_predict(x) > self.activation_threshold else -1\n",
    "        \n",
    "    def set_activation_threshold(self, value):\n",
    "        self.activation_threshold = value\n",
    "        \n",
    "    def plot_roc(self, labeled_set, point_count=150):\n",
    "        current_threshold = self.activation_threshold\n",
    "        tpr = []\n",
    "        fpr = []\n",
    "        for i in range(point_count, -1 - point_count, -2):\n",
    "            self.activation_threshold = i / point_count\n",
    "            tpr.append(self.recall(labeled_set))\n",
    "            fpr.append(self.fall_out(labeled_set))\n",
    "        plt.scatter(fpr, tpr)\n",
    "        plt.show()\n",
    "        auc_roc = 0\n",
    "        for i in range(1, len(tpr)):\n",
    "            auc_roc += (min(tpr[i], tpr[i - 1]) * (fpr[i] - fpr[i - 1])\n",
    "                        + (max(tpr[i], tpr[i - 1]) - min(tpr[i], tpr[i - 1])) * (fpr[i] - fpr[i - 1]) / 2)\n",
    "        self.activation_threshold = current_threshold\n",
    "        return auc_roc\n",
    "\n",
    "class DecisionTree(Classifier):\n",
    "    class Vertex:\n",
    "        def __init__(self):\n",
    "            self.is_leaf = True\n",
    "        \n",
    "        def create_descendants(self, dimension, separator):\n",
    "            self.dimension = dimension\n",
    "            self.separator = separator\n",
    "            self.left = DecisionTree.Vertex()\n",
    "            self.right = DecisionTree.Vertex()\n",
    "            self.is_leaf = False\n",
    "            return self.left, self.right\n",
    "        \n",
    "        def set_label(self, label):\n",
    "            self.label = label\n",
    "        \n",
    "        def proceed(self, input_vector, find_index=False):\n",
    "            if self.is_leaf:\n",
    "                if find_index:\n",
    "                    return (self.label, self.index)\n",
    "                return self.label\n",
    "            if input_vector[self.dimension] > self.separator:\n",
    "                return self.right.proceed(input_vector, find_index)\n",
    "            else:\n",
    "                return self.left.proceed(input_vector, find_index)\n",
    "        \n",
    "        def copy(self):\n",
    "            vertex = Vertex()\n",
    "            if self.is_leaf:\n",
    "                vertex.set_label(self.label)\n",
    "            else:\n",
    "                vertex.left = self.left.copy()\n",
    "                vertex.right = self.right.copy()\n",
    "                vertex.dimension = dimension\n",
    "                vertex.separator = separator\n",
    "        \n",
    "        # fonction pour TreeBoost \n",
    "        def set_leaf_index(self, index):\n",
    "            if self.is_leaf:\n",
    "                self.index = index\n",
    "                return index + 1\n",
    "            index = self.left.set_leaf_index(index)\n",
    "            index = self.right.set_leaf_index(index)\n",
    "            return index\n",
    "    \n",
    "    def __init__(self, leaf_threshold, metrics=\"Shannon\", vertex_possible_dimensions=-1):\n",
    "        self.vertex_possible_dimensions = vertex_possible_dimensions\n",
    "        self.leaf_threshold = leaf_threshold\n",
    "        if metrics != \"Shannon\" and metrics != \"Gini\" and metrics != \"Variance\":\n",
    "            raise ValueError(\"Unrecognized metrics: \" + metrics)\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def count_results(self, data_set):\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        for output in data_set.y:\n",
    "            if output > 0:\n",
    "                positive_count += 1\n",
    "            else:\n",
    "                negative_count += 1\n",
    "        return positive_count, negative_count\n",
    "    \n",
    "    #### Metrics ####\n",
    "    \n",
    "    def gini_metrics(self, positive_count, negative_count):\n",
    "        total_count = positive_count + negative_count\n",
    "        return (1\n",
    "                - (positive_count / total_count) ** 2\n",
    "                - (negative_count / total_count) ** 2) \n",
    "    \n",
    "    def measure_set_gini_metrics(self, data_set):\n",
    "        positive_count, negative_count = self.count_results(data_set)\n",
    "        return self.gini_metrics(positive_count, negative_count)\n",
    "    \n",
    "    def entropy(self, positive_count, negative_count):\n",
    "        positive_probability = positive_count / (positive_count + negative_count)\n",
    "        if positive_probability == 0 or positive_probability == 1:\n",
    "            return 0\n",
    "        return -(positive_probability * log(positive_probability, 2)\n",
    "                + (1 - positive_probability) * log(1 - positive_probability, 2))\n",
    "    \n",
    "    def measure_set_entropy(self, data_set):\n",
    "        positive_count, negative_count = self.count_results(data_set)\n",
    "        return self.entropy(positive_count, negative_count)\n",
    "    \n",
    "    def variance(self, data_set):  # pour la regression\n",
    "        return data_set.y.var() * data_set.size()\n",
    "        \n",
    "    ##################\n",
    "    \n",
    "    def predict(self, x, find_index=False):\n",
    "        return self.root.proceed(x, find_index)\n",
    "\n",
    "    def partition(self, data_set, separator_dimension, separator):\n",
    "        left_set = LabeledSet(self.input_dimension)\n",
    "        right_set = LabeledSet(self.input_dimension)\n",
    "        for i in range(data_set.x.shape[0]):\n",
    "            if data_set.x[i, separator_dimension] <= separator:\n",
    "                left_set.add_example(data_set.x[i], data_set.y[i])\n",
    "            else:\n",
    "                right_set.add_example(data_set.x[i], data_set.y[i])\n",
    "        return left_set, right_set\n",
    "    \n",
    "    def move_partition(self, data_set, dimension):\n",
    "        sorted_indexes = np.argsort(data_set.x[:, dimension])\n",
    "        positive_count, negative_count = 0, 0\n",
    "        for index in range(sorted_indexes.shape[0] - 1):\n",
    "            if data_set.y[sorted_indexes[index]] > 0:\n",
    "                positive_count += 1\n",
    "            else:\n",
    "                negative_count += 1\n",
    "            if np.array_equal(data_set.x[sorted_indexes[index], dimension], data_set.x[sorted_indexes[index + 1], dimension]):\n",
    "                continue\n",
    "            yield (positive_count,\n",
    "                   negative_count,\n",
    "                   (data_set.x[sorted_indexes[index], dimension] + data_set.x[sorted_indexes[index + 1], dimension]) / 2)\n",
    "    \n",
    "    def find_classification_best_axis_partition(self,\n",
    "                                                current_set,\n",
    "                                                dimension,\n",
    "                                                current_min_dimension,\n",
    "                                                current_min_separator,\n",
    "                                                current_min_metrics):\n",
    "        total_positives, total_negatives = self.count_results(current_set)\n",
    "        total_count = current_set.x.shape[0]\n",
    "        left_side_count, right_side_count = (0, 0)\n",
    "        metrics = 0\n",
    "        for positive_count, negative_count, separator in self.move_partition(current_set, dimension):\n",
    "            left_side_count = positive_count + negative_count\n",
    "            right_side_count = current_set.x.shape[0] - left_side_count\n",
    "            if self.metrics == \"Shannon\":\n",
    "                metrics = (self.entropy(positive_count, negative_count) * left_side_count / total_count +\n",
    "                                     self.entropy(total_positives - positive_count, total_negatives - negative_count) * right_side_count / total_count) / 2\n",
    "            elif self.metrics == \"Gini\":\n",
    "                metrics = (self.gini_metrics(positive_count, negative_count) * left_side_count / total_count +\n",
    "                                     self.gini_metrics(total_positives - positive_count, total_negatives - negative_count) * right_side_count / total_count) / 2\n",
    "            if metrics < current_min_metrics:\n",
    "                current_min_metrics = metrics\n",
    "                current_min_dimension = dimension\n",
    "                current_min_separator = separator\n",
    "        return current_min_dimension, current_min_separator, current_min_metrics\n",
    "    \n",
    "    def find_regression_best_axis_partition(self,\n",
    "                                            current_set,\n",
    "                                            dimension,\n",
    "                                            current_min_dimension,\n",
    "                                            current_min_separator,\n",
    "                                            current_min_metrics):\n",
    "        metrics = 0\n",
    "        for separator in np.linspace(current_set.x[:, dimension].min(),\n",
    "                                     current_set.x[:, dimension].max(),\n",
    "                                     self.possible_partition_splits_count + 1):\n",
    "            left_set, right_set = self.partition(current_set, dimension, separator)\n",
    "            if left_set.size() == 0 or right_set.size() == 0:\n",
    "                continue\n",
    "            if self.metrics == \"Variance\":\n",
    "                metrics = self.variance(left_set) + self.variance(right_set)\n",
    "            if metrics < current_min_metrics:\n",
    "                current_min_metrics = metrics\n",
    "                current_min_dimension = dimension\n",
    "                current_min_separator = separator\n",
    "        return current_min_dimension, current_min_separator, current_min_metrics\n",
    "    \n",
    "    def find_best_partition(self, current_set):\n",
    "        current_min_metrics = 2\n",
    "        if self.metrics == \"Variance\":\n",
    "            current_min_metrics = self.variance(current_set)\n",
    "        current_min_dimension = -1\n",
    "        current_min_separator = -1\n",
    "        dimensions = []\n",
    "        if self.vertex_possible_dimensions == -1:\n",
    "            dimensions = range(self.input_dimension)\n",
    "        else:\n",
    "            dimensions = random.sample(range(self.input_dimension), self.vertex_possible_dimensions)\n",
    "        for dimension in range(self.input_dimension):\n",
    "            if self.metrics == \"Shannon\" or self.metrics == \"Gini\":\n",
    "                (\n",
    "                    current_min_dimension,\n",
    "                    current_min_separator,\n",
    "                    current_min_metrics\n",
    "                ) = self.find_classification_best_axis_partition(current_set,\n",
    "                                                            dimension,\n",
    "                                                            current_min_dimension,\n",
    "                                                            current_min_separator,\n",
    "                                                            current_min_metrics)\n",
    "            elif self.metrics == \"Variance\":\n",
    "                (\n",
    "                    current_min_dimension,\n",
    "                    current_min_separator,\n",
    "                    current_min_metrics\n",
    "                ) = self.find_regression_best_axis_partition(current_set,\n",
    "                                                        dimension,\n",
    "                                                        current_min_dimension,\n",
    "                                                        current_min_separator,\n",
    "                                                        current_min_metrics)\n",
    "        return current_min_dimension, current_min_separator\n",
    "    \n",
    "    def set_leaf_label(self, leaf, current_set):\n",
    "        if self.metrics == \"Variance\":\n",
    "            leaf.set_label(current_set.y.mean())\n",
    "            return\n",
    "        leaf.set_label(1 if current_set.y.mean() > 0 else -1)\n",
    "                    \n",
    "    def recurrent_train(self, current_set, current_vertex, current_depth):\n",
    "        if (self.metrics == \"Shannon\" and self.measure_set_entropy(current_set) <= self.leaf_threshold\n",
    "               or self.metrics == \"Gini\" and self.measure_set_gini_metrics(current_set) <= self.leaf_threshold):\n",
    "            self.set_leaf_label(current_vertex, current_set)\n",
    "            return\n",
    "        if (self.metrics == \"Variance\" and self.variance(current_set) <= self.leaf_threshold):\n",
    "            self.set_leaf_label(current_vertex, current_set)\n",
    "            return\n",
    "        partition_parameters = self.find_best_partition(current_set)\n",
    "        if partition_parameters[0] == -1:\n",
    "            self.set_leaf_label(current_vertex, current_set)\n",
    "            return\n",
    "        left_vertex, right_vertex = current_vertex.create_descendants(partition_parameters[0],\n",
    "                                                                      partition_parameters[1])\n",
    "        subsets = self.partition(current_set, partition_parameters[0], partition_parameters[1])\n",
    "        if self.max_depth == -1 or current_depth < self.max_depth:\n",
    "            self.recurrent_train(subsets[0], left_vertex, current_depth + 1)\n",
    "            self.recurrent_train(subsets[1], right_vertex, current_depth + 1)\n",
    "        else:\n",
    "            self.set_leaf_label(left_vertex, subsets[0])\n",
    "            self.set_leaf_label(right_vertex, subsets[1])\n",
    "    \n",
    "    def train(self, training_set, max_depth=-1, possible_partition_splits_count=15):\n",
    "        self.input_dimension = training_set.x.shape[1]\n",
    "        self.possible_partition_splits_count = possible_partition_splits_count\n",
    "        self.root = DecisionTree.Vertex()\n",
    "        self.max_depth = max_depth\n",
    "        self.recurrent_train(training_set, self.root, 0)\n",
    "\n",
    "\n",
    "class RandomForest(Classifier):\n",
    "    def __init__(self, tree_count, tree_metrics=\"Gini\"):\n",
    "        self.tree_count = tree_count\n",
    "        self.tree_metrics = tree_metrics\n",
    "\n",
    "    def get_tree_count(self):\n",
    "        return self.tree_count\n",
    "\n",
    "    def get_tree_outputs(self, input_vector):\n",
    "        return [self.trees[i].predict(x[self.trees_dimensions[i]]) for i in range(self.get_tree_count())]\n",
    "\n",
    "    def predict(self, x):\n",
    "        positive_trees_count = 0\n",
    "        for i in range(self.tree_count):\n",
    "            if self.trees[i].predict(x) == 1:#[self.trees_dimensions[i]]) == 1:\n",
    "                positive_trees_count += 1\n",
    "        return 1 if positive_trees_count > self.tree_count / 2 else -1\n",
    "\n",
    "    def get_tree_input_dimesion(self, forest_input_dimension):\n",
    "        return max(forest_input_dimension, 1)\n",
    "\n",
    "    def train(self, labeled_set, max_depth=-1, verbose=False):\n",
    "        if labeled_set.size() == 0:\n",
    "            raise RuntimeError(\"Empty training set!\")\n",
    "        self.input_dimension = labeled_set.x.shape[1]\n",
    "        self.tree_input_dimesion = self.get_tree_input_dimesion(self.input_dimension)\n",
    "        self.trees = []\n",
    "        chosen_example_index = 0\n",
    "        for i in range(self.tree_count):\n",
    "            self.trees.append(DecisionTree(0.0,\n",
    "                                           metrics=self.tree_metrics,\n",
    "                                           vertex_possible_dimensions=self.tree_input_dimesion))\n",
    "            training_subset = LabeledSet(self.tree_input_dimesion)\n",
    "            for j in range(labeled_set.size()):\n",
    "                chosen_example_index = np.random.randint(labeled_set.size())\n",
    "                training_subset.add_example(\n",
    "                    labeled_set.get_x(chosen_example_index) ,\n",
    "                    labeled_set.get_y(chosen_example_index)\n",
    "                )\n",
    "            self.trees[i].train(training_subset, max_depth=max_depth)\n",
    "            if verbose:\n",
    "                print(\"trees trained:\", (i + 1), \"/\", self.tree_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "def loadFile(name):\n",
    "    data = fetch_mldata(name, data_home='.')\n",
    "    unique = np.unique(data.target)\n",
    "    for i in range(len(data.target)):\n",
    "        if (data.target[i]==unique[0]):\n",
    "            data.target[i]=1\n",
    "        else:\n",
    "            data.target[i]=-1\n",
    "    train_data = LabeledSet(data)\n",
    "    for i in range(data.data.shape[0] // 2):\n",
    "        train_data.add_example(data.data[i], data.target[i])\n",
    "    test_data = LabeledSet(data)\n",
    "    for i in range(data.data.shape[0] // 2, data.data.shape[0]):\n",
    "        test_data.add_example(data.data[i], data.target[i])\n",
    "    return (train_data, test_data)\n",
    "            \n",
    "train_data, test_data = loadFile('breast-cancer_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_data_train = generate_data_set()\n",
    "g_data_test = generate_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6981a196be72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom_forest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_data_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_frontiere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_data_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot2DSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_data_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d9961eb0b4ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, labeled_set, max_depth, verbose)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0mlabeled_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_example_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 )\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trees trained:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d9961eb0b4ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_set, max_depth, possible_partition_splits_count)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVertex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d9961eb0b4ed>\u001b[0m in \u001b[0;36mrecurrent_train\u001b[0;34m(self, current_set, current_vertex, current_depth)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_vertex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vertex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_leaf_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_vertex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d9961eb0b4ed>\u001b[0m in \u001b[0;36mrecurrent_train\u001b[0;34m(self, current_set, current_vertex, current_depth)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_leaf_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_vertex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mpartition_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpartition_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_leaf_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_vertex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d9961eb0b4ed>\u001b[0m in \u001b[0;36mfind_best_partition\u001b[0;34m(self, current_set)\u001b[0m\n\u001b[1;32m    298\u001b[0m                                                             \u001b[0mcurrent_min_dimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                                                             \u001b[0mcurrent_min_separator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                                                             current_min_metrics)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Variance\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 (\n",
      "\u001b[0;32m<ipython-input-2-d9961eb0b4ed>\u001b[0m in \u001b[0;36mfind_classification_best_axis_partition\u001b[0;34m(self, current_set, dimension, current_min_dimension, current_min_separator, current_min_metrics)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mleft_side_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_side_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpositive_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0mleft_side_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositive_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegative_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mright_side_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mleft_side_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d9961eb0b4ed>\u001b[0m in \u001b[0;36mmove_partition\u001b[0;34m(self, data_set, dimension)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mnegative_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msorted_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msorted_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             yield (positive_count,\n",
      "\u001b[0;32m/home/arsen/.local/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36marray_equal\u001b[0;34m(a1, a2)\u001b[0m\n\u001b[1;32m   2621\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsen/.local/lib/python3.5/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_all\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_forest = RandomForest(80)\n",
    "random_forest.train(g_data_train, max_depth=3)\n",
    "plot_frontiere(g_data_test, random_forest, step=50)\n",
    "plot2DSet(g_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientBoost(Regression):\n",
    "    class ConstantClassifier(Classifier):\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        \n",
    "        def train(self, training_set):\n",
    "            self.result = training_set.y.mean()\n",
    "            \n",
    "        def predict(self, x):\n",
    "            return self.result\n",
    "    \n",
    "    def __init__(self,\n",
    "                 BaseClassifier):\n",
    "        self.BaseClassifier = BaseClassifier\n",
    "        self.activation_threshold = 0.0\n",
    "    \n",
    "    def set_activation_threshold(self, value, ):\n",
    "        self.activation_threshold = value\n",
    "    \n",
    "    def regression_predict(self, x):\n",
    "        return self.coefficients.dot([self.classifiers[i].predict(x) for i in range(len(self.classifiers))]) > self.activation_threshold\n",
    "    \n",
    "    def limited_prediction(self, x):  # utilisée pendant l'apprentissage\n",
    "        return self.coefficients[:len(self.classifiers) - 1].dot([self.classifiers[i].predict(x) for i in range(len(self.classifiers) - 1)])\n",
    "    \n",
    "    def get_pseudo_residuals(self, loss_function_derivative, training_set):\n",
    "        result = LabeledSet(training_set.x.shape[1])\n",
    "        for example_index in range(training_set.size()):\n",
    "            result.add_example(\n",
    "                training_set.get_x(example_index),\n",
    "                (-loss_function_derivative(self.limited_prediction(training_set.get_x(example_index)),\n",
    "                                          training_set.get_y(example_index)))\n",
    "            )\n",
    "        return result\n",
    "            \n",
    "    def coefficient_gradient_descent(self,\n",
    "                                     training_set,\n",
    "                                     coefficient_index,\n",
    "                                     loss_function_derivative,\n",
    "                                     learning_rate,\n",
    "                                     gradient_step_learning_rate,\n",
    "                                     learn_threshold,\n",
    "                                     max_descent_iterations):\n",
    "        difference = learn_threshold + 0.000001\n",
    "        classifier_prediction = 0.0\n",
    "        previous_model_prediction = 0.0\n",
    "        inital_coefficient = self.coefficients[coefficient_index]\n",
    "        iteration = 0\n",
    "        while abs(difference) > learn_threshold and max_descent_iterations > iteration:\n",
    "            self.coefficients[coefficient_index] += difference\n",
    "            difference = 0.0\n",
    "            for example_index in range(training_set.size()):\n",
    "                previous_model_prediction = self.limited_prediction(training_set.get_x(example_index))\n",
    "                classifier_prediction = self.classifiers[-1].predict(training_set.get_x(example_index))\n",
    "                difference -= (learning_rate\n",
    "                               * classifier_prediction\n",
    "                               * loss_function_derivative(classifier_prediction * self.coefficients[coefficient_index]\n",
    "                                                          + previous_model_prediction,\n",
    "                                                          training_set.get_y(example_index)))\n",
    "            iteration += 1\n",
    "        self.coefficients[coefficient_index] = (inital_coefficient\n",
    "                                                + gradient_step_learning_rate\n",
    "                                                * (self.coefficients[coefficient_index] - inital_coefficient))\n",
    "    \n",
    "    def train(self,\n",
    "              training_set,\n",
    "              learning_rate,\n",
    "              learn_threshold,\n",
    "              gradient_step_learning_rate,\n",
    "              classifier_train_kwargs,\n",
    "              classifier_init_args,\n",
    "              classifier_init_kwargs,\n",
    "              loss_function_derivative,  # d(L(ŷ, y)) / d(ŷ)\n",
    "              iteration_count,\n",
    "              max_descent_iterations=100000,\n",
    "              verbose=False):\n",
    "        self.classifiers = [self.ConstantClassifier()]\n",
    "        self.classifiers[0].train(training_set)\n",
    "        self.coefficients = np.zeros(iteration_count)\n",
    "        self.coefficients[0] = 1\n",
    "        for iteration in range(1, iteration_count):\n",
    "            if verbose:\n",
    "                print(\"Started iteration \" + str(iteration))\n",
    "            self.classifiers.append(self.BaseClassifier(*classifier_init_args, **classifier_init_kwargs))\n",
    "            current_training_set = self.get_pseudo_residuals(loss_function_derivative, training_set)\n",
    "            self.classifiers[-1].train(current_training_set, **classifier_train_kwargs)\n",
    "            if verbose:\n",
    "                print(\"Basic classifier trained\")\n",
    "            self.coefficient_gradient_descent(training_set,\n",
    "                                              iteration,\n",
    "                                              loss_function_derivative,\n",
    "                                              learning_rate,\n",
    "                                              gradient_step_learning_rate,\n",
    "                                              learn_threshold,\n",
    "                                              max_descent_iterations)\n",
    "        if verbose:\n",
    "            print(\"Finished training\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_function(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def least_squares_derivative(prediction, y):\n",
    "    return 2 * (prediction - y)\n",
    "\n",
    "def logistic_function_maximizer(prediction, y):\n",
    "    return (logistic_function(prediction) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Perceptron(Classifier):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def set_iteration_count(self, iteration_count):\n",
    "        self.iteration_count = iteration_count\n",
    "        \n",
    "    def get_iteration_count(self):\n",
    "        return self.iteration_count\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.activation_function(self.theta.dot(self.kernel_function(x)))\n",
    "    \n",
    "    def _predict_no_kernel(self, x):\n",
    "        return self.activation_function(self.theta.dot(x))\n",
    "    \n",
    "    # adjust value of theta according to value of input vector\n",
    "    def vector_train(self, input_vector, expected_output):\n",
    "        self.theta -= (input_vector\n",
    "                       * self.loss_function_derivative(self._predict_no_kernel(input_vector), expected_output)\n",
    "                       * self.learning_rate)\n",
    "    \n",
    "    def train_iteration(self, labeledSet):\n",
    "        chosen_index = np.random.randint(labeledSet.x.shape[0])\n",
    "        self.vector_train(self.kernel_function(labeledSet.x[chosen_index]), labeledSet.y[chosen_index])\n",
    "    \n",
    "    def train(self,\n",
    "              labeled_set,\n",
    "              learning_rate,\n",
    "              loss_function_derivative,\n",
    "              iteration_count=10000,\n",
    "              kernel_function=lambda x: x,\n",
    "              activation_function=lambda x: 1 if x > 0 else -1):\n",
    "        self.iteration_count = iteration_count\n",
    "        self.learning_rate = learning_rate\n",
    "        self.data_dimension = kernel_function(labeled_set.x[0]).shape[0]\n",
    "        self.theta = np.ones(self.data_dimension)\n",
    "        chosen_index = 0\n",
    "        self.kernel_function = kernel_function\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function_derivative = loss_function_derivative\n",
    "        for i in range(self.iteration_count):\n",
    "            self.train_iteration(labeled_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.train(g_data_train,\n",
    "                 0.001,\n",
    "                 logistic_function_maximizer,\n",
    "                 iteration_count=50000,\n",
    "                 kernel_function=lambda x: np.hstack((x, 1)))#,\n",
    "                 #activation_function=logistic_function)\n",
    "plot_frontiere(g_data_test, perceptron, step=50)\n",
    "plot2DSet(g_data_test)\n",
    "print(perceptron.accuracy(g_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient_boost = GradientBoost(Perceptron)\n",
    "gradient_boost.train(g_data_train,\n",
    "                     0.0001,\n",
    "                     0.00001,\n",
    "                     0.1,\n",
    "                     {\"learning_rate\": 0.001,\n",
    "                      \"iteration_count\": 25000,\n",
    "                      \"kernel_function\": lambda x: np.hstack((x, 1)),\n",
    "                      \"loss_function_derivative\": least_squares_derivative},\n",
    "                      #\"activation_function\": lambda x: x},#logistic_function},\n",
    "                     (),\n",
    "                     {},\n",
    "                     least_squares_derivative,  # d(L(ŷ, y)) / d(ŷ)\n",
    "                     25,\n",
    "                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient_boost.set_activation_threshold(0.062345679)\n",
    "plot_frontiere(g_data_test, gradient_boost, step=50)\n",
    "plot2DSet(g_data_test)\n",
    "print(gradient_boost.accuracy(g_data_test))\n",
    "#print(gradient_boost.precision(test_data))\n",
    "#print(gradient_boost.recall(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient_boost.plot_roc(g_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TreeBoost(Regression):\n",
    "    def __init__(self):\n",
    "        self.BaseClassifier = DecisionTree\n",
    "        self.activation_threshold = 0.0\n",
    "        \n",
    "    def limited_prediction(self, x):  # utilisée pendant l'apprentissage\n",
    "        leaves_indexes = [0]\n",
    "        for i in range(1, len(self.classifiers) - 1):\n",
    "            leaves_indexes.append(self.classifiers[i].predict(x, find_index=True)[1])\n",
    "        return self.coefficients[leaves_indexes].sum()\n",
    "    \n",
    "    def get_pseudo_residuals(self, loss_function_derivative, training_set):\n",
    "        result = LabeledSet(training_set.x.shape[1])\n",
    "        for example_index in range(training_set.size()):\n",
    "            result.add_example(\n",
    "                training_set.get_x(example_index),\n",
    "                (-loss_function_derivative(self.limited_prediction(training_set.get_x(example_index)),\n",
    "                                          training_set.get_y(example_index)))\n",
    "            )\n",
    "        return result\n",
    "    \n",
    "    def regression_predict(self, x):\n",
    "        leaves_indexes = [0]\n",
    "        for i in range(1, len(self.classifiers)):\n",
    "            leaves_indexes.append(self.classifiers[i].predict(x, find_index=True)[1])\n",
    "        return self.coefficients[leaves_indexes].sum() > self.activation_threshold\n",
    "    \n",
    "    def coefficient_gradient_descent(self,\n",
    "                                     training_set,\n",
    "                                     coefficient_index,\n",
    "                                     loss_function_derivative,\n",
    "                                     learning_rate,\n",
    "                                     gradient_step_learning_rate,\n",
    "                                     learn_threshold,\n",
    "                                     max_descent_iterations):\n",
    "        difference = (learn_threshold + 0.1) * np.ones(self.coefficients.shape[0] - self.previous_model_leaves_count)\n",
    "        #inital_coefficients = self.coefficients[self.previous_model_leaves_count:]\n",
    "        classifier_prediction = 0.0\n",
    "        previous_model_prediction = 0.0\n",
    "        iteration = 0\n",
    "        while difference.dot(difference) > learn_threshold and max_descent_iterations > iteration:\n",
    "            for i in range(difference.shape[0]):\n",
    "                self.coefficients[self.previous_model_leaves_count + i] += difference[i] * gradient_step_learning_rate\n",
    "                difference[i] = 0\n",
    "            for example_index in range(training_set.size()):\n",
    "                previous_model_prediction = self.limited_prediction(training_set.get_x(example_index))\n",
    "                classifier_prediction = self.classifiers[-1].predict(training_set.get_x(example_index), find_index=True)\n",
    "                difference[classifier_prediction[1] - self.previous_model_leaves_count] -= (\n",
    "                    learning_rate\n",
    "                    * loss_function_derivative(self.coefficients[classifier_prediction[1]]\n",
    "                                               + previous_model_prediction,\n",
    "                                               training_set.get_y(example_index))\n",
    "                )\n",
    "                iteration += 1\n",
    "                \n",
    "    def get_pseudo_residuals(self, loss_function_derivative, training_set):\n",
    "        result = LabeledSet(training_set.x.shape[1])\n",
    "        for example_index in range(training_set.size()):\n",
    "            result.add_example(\n",
    "                training_set.get_x(example_index),\n",
    "                (-loss_function_derivative(self.limited_prediction(training_set.get_x(example_index)),\n",
    "                                          training_set.get_y(example_index)))\n",
    "            )\n",
    "        return result\n",
    "            \n",
    "    def train(self,\n",
    "              training_set,\n",
    "              learning_rate,\n",
    "              learn_threshold,\n",
    "              gradient_step_learning_rate,\n",
    "              classifier_train_kwargs,\n",
    "              classifier_init_args,\n",
    "              classifier_init_kwargs,\n",
    "              loss_function_derivative,  # d(L(ŷ, y)) / d(ŷ)\n",
    "              iteration_count,\n",
    "              max_descent_iterations=500000,\n",
    "              verbose=False):\n",
    "        self.classifiers = [None]\n",
    "        self.coefficients = np.zeros(1)\n",
    "        self.coefficients[0] = training_set.y.mean()\n",
    "        self.previous_model_leaves_count = 1\n",
    "        for iteration in range(1, iteration_count + 1):\n",
    "            if verbose:\n",
    "                print(\"Started iteration \" + str(iteration))\n",
    "            self.classifiers.append(DecisionTree(*classifier_init_args, **classifier_init_kwargs))\n",
    "            current_training_set = self.get_pseudo_residuals(loss_function_derivative, training_set)\n",
    "            self.classifiers[-1].train(current_training_set, **classifier_train_kwargs)\n",
    "            new_leaves_count = self.classifiers[-1].root.set_leaf_index(self.previous_model_leaves_count) - self.previous_model_leaves_count\n",
    "            self.coefficients = np.hstack((self.coefficients, np.zeros(new_leaves_count)))\n",
    "            if verbose:\n",
    "                print(\"Tree is trained, calculating coefficient...\")\n",
    "            self.coefficient_gradient_descent(training_set,\n",
    "                                              iteration,\n",
    "                                              loss_function_derivative,\n",
    "                                              learning_rate,\n",
    "                                              gradient_step_learning_rate,\n",
    "                                              learn_threshold,\n",
    "                                              max_descent_iterations)\n",
    "            self.previous_model_leaves_count += new_leaves_count\n",
    "            if verbose:\n",
    "                print(\"\")\n",
    "        if verbose:\n",
    "            print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_boost = TreeBoost()\n",
    "tree_boost.train(train_data,\n",
    "                 0.0001,\n",
    "                 0.000000001,\n",
    "                 0.05,\n",
    "                 {\"max_depth\": 8},\n",
    "                 (0.,),\n",
    "                 {\"metrics\": \"Gini\"},\n",
    "                 least_squares_derivative,  # d(L(ŷ, y)) / d(ŷ)\n",
    "                 25,\n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_boost.set_activation_threshold(0.00073)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"accuracy: \", tree_boost.accuracy(test_data))\n",
    "print(\"precision: \", tree_boost.precision(test_data))\n",
    "print(\"recall: \", tree_boost.recall(test_data))\n",
    "print(\"f_measure: \", tree_boost.f_measure(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTree(0., metrics=\"Gini\")\n",
    "tree.train(train_data, max_depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tree.recall(test_data))\n",
    "print(tree.precision(test_data))\n",
    "print(tree.accuracy(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
